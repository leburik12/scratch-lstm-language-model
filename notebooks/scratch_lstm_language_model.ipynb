{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import math\n",
        "from google.colab import files\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "C88iB7uWA_hX"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = \"/content/pride_and_prejudice.txt\"\n",
        "\n",
        "try:\n",
        "  with open(file_path, \"r\", encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "    print(f\"Successfully read file from: {file_path}\")\n",
        "    print(f\"Dataset length: {len(text)} Characters\")\n",
        "except FileNotFoundError:\n",
        "  print(f\"Error: File not found at {file_path}. Did you upload it or mount Google Drive correctly?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o84pnfSXNic7",
        "outputId": "0becac42-1e32-4ebe-d923-76a75e390fe3"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully read file from: /content/pride_and_prejudice.txt\n",
            "Dataset length: 748135 Characters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def generate_text(model, output_layer, stoi, itos, start_char=\"H\",\n",
        "                  length=300, temperature=1.0, device=\"cpu\"):\n",
        "    model.eval()\n",
        "\n",
        "    idx = torch.tensor([stoi[start_char]], device=device)\n",
        "    x = F.one_hot(idx, num_classes=len(stoi)).float().unsqueeze(0).unsqueeze(1)\n",
        "\n",
        "    H_C = None\n",
        "    generated = [start_char]\n",
        "\n",
        "    for _ in range(length):\n",
        "        outputs, H_C = model(x, H_C)\n",
        "\n",
        "        logits = output_layer(outputs[-1])      # (1, V)\n",
        "        logits = logits.squeeze(0)              # (V,)\n",
        "        logits /= temperature\n",
        "\n",
        "        probs = F.softmax(logits, dim=-1)       # (V,)\n",
        "\n",
        "        idx = torch.multinomial(probs.view(-1), 1)  # ‚úÖ FIX\n",
        "        char = itos[idx.item()]\n",
        "        generated.append(char)\n",
        "\n",
        "        x = F.one_hot(idx, num_classes=len(stoi)).float().unsqueeze(0).unsqueeze(1)\n",
        "\n",
        "    return \"\".join(generated)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# @torch.no_grad()\n",
        "# def generate_text(model, output_layer, stoi, itos, start_char=\"H\",\n",
        "#                   length=300, temperature=1.0, device=\"cpu\"):\n",
        "#     model.eval()\n",
        "\n",
        "#     idx = torch.tensor([stoi[start_char]], device=device)\n",
        "#     x = F.one_hot(idx, num_classes=len(stoi)).float().unsqueeze(0).unsqueeze(1)\n",
        "\n",
        "#     H_C = None\n",
        "#     generated = [start_char]\n",
        "\n",
        "#     for _ in range(length):\n",
        "#         outputs, H_C = model(x, H_C)\n",
        "#         logits = output_layer(outputs[-1]).squeeze(0)\n",
        "#         logits /= temperature\n",
        "#         probs = F.softmax(logits, dim=-1)\n",
        "\n",
        "#         idx = torch.multinomial(probs, 1)\n",
        "#         char = itos[idx.item()]\n",
        "#         generated.append(char)\n",
        "\n",
        "#         x = F.one_hot(idx, num_classes=len(stoi)).float().unsqueeze(0).unsqueeze(1)\n",
        "\n",
        "#     return \"\".join(generated)"
      ],
      "metadata": {
        "id": "w6DnuybCGwTX"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "dYsQ12k3NSv0"
      },
      "outputs": [],
      "source": [
        "class LSTMScratch(nn.Module):\n",
        "    \"\"\"\n",
        "    A minimal, from-scratch implementation of the Long Short-Term Memory (LSTM) unit.\n",
        "    The implementation handles the forward pass for an entire sequence.\n",
        "    \"\"\"\n",
        "    def __init__(self, num_inputs: int, num_hiddens: int, sigma: float = 0.01):\n",
        "        \"\"\"\n",
        "        Initializes all weight matrices and bias vectors for the four gates/nodes:\n",
        "        Input (I), Forget (F), Output (O), and Candidate Cell (C_tilde).\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.num_hiddens = num_hiddens\n",
        "        self.num_inputs = num_inputs\n",
        "\n",
        "        # Utility function to initialize weights (X -> Hidden, H -> Hidden) and bias\n",
        "        def init_weights_and_bias():\n",
        "            # W_x (Input to Hidden)\n",
        "            W_x = nn.Parameter(torch.randn(num_inputs, num_hiddens) * sigma)\n",
        "            # W_h (Hidden to Hidden, Recurrent)\n",
        "            W_h = nn.Parameter(torch.randn(num_hiddens, num_hiddens) * sigma)\n",
        "            # Bias (b)\n",
        "            b = nn.Parameter(torch.zeros(num_hiddens))\n",
        "            return W_x, W_h, b\n",
        "\n",
        "        # --- Gates and Nodes Initialization ---\n",
        "\n",
        "        # 1. Input Gate (I): Controls how much the candidate value updates the cell state\n",
        "        self.W_xi, self.W_hi, self.b_i = init_weights_and_bias()\n",
        "\n",
        "        # 2. Forget Gate (F): Controls how much of the old cell state (C) is retained\n",
        "        self.W_xf, self.W_hf, self.b_f = init_weights_and_bias()\n",
        "\n",
        "        # 3. Output Gate (O): Controls how much of the cell state (C) is exposed to the hidden state (H)\n",
        "        self.W_xo, self.W_ho, self.b_o = init_weights_and_bias()\n",
        "\n",
        "        # 4. Candidate Cell (C_tilde): The new information proposed for the cell state\n",
        "        self.W_xc, self.W_hc, self.b_c = init_weights_and_bias()\n",
        "\n",
        "\n",
        "    def forward(self, inputs: torch.Tensor, H_C: tuple[torch.Tensor, torch.Tensor] = None):\n",
        "        \"\"\"\n",
        "        Performs the forward pass over the sequence.\n",
        "\n",
        "        Args:\n",
        "            inputs: Tensor of shape (num_steps, batch_size, num_inputs).\n",
        "            H_C: Optional tuple (H, C) of initial hidden state and cell state.\n",
        "\n",
        "        Returns:\n",
        "            A tuple (outputs, (H, C)) containing:\n",
        "            - outputs: List of hidden states H for each time step.\n",
        "            - (H, C): The final hidden and cell states of the sequence.\n",
        "        \"\"\"\n",
        "        # Determine initial states (H and C)\n",
        "        if H_C is None:\n",
        "            batch_size = inputs.shape[1]\n",
        "            device = inputs.device\n",
        "            H = torch.zeros((batch_size, self.num_hiddens), device=device)\n",
        "            C = torch.zeros((batch_size, self.num_hiddens), device=device)\n",
        "        else:\n",
        "            H, C = H_C\n",
        "\n",
        "        outputs = []\n",
        "\n",
        "        # Iterate over the sequence (inputs[0] is X_1, inputs[1] is X_2, etc.)\n",
        "        for X in inputs:\n",
        "            # 1. Input Gate (I_t): Uses sigmoid activation\n",
        "            I = torch.sigmoid(torch.matmul(X, self.W_xi) +\n",
        "                             torch.matmul(H, self.W_hi) + self.b_i)\n",
        "\n",
        "            # 2. Forget Gate (F_t): Uses sigmoid activation\n",
        "            F = torch.sigmoid(torch.matmul(X, self.W_xf) +\n",
        "                             torch.matmul(H, self.W_hf) + self.b_f)\n",
        "\n",
        "            # 3. Output Gate (O_t): Uses sigmoid activation\n",
        "            O = torch.sigmoid(torch.matmul(X, self.W_xo) +\n",
        "                             torch.matmul(H, self.W_ho) + self.b_o)\n",
        "\n",
        "            # 4. Candidate Cell (~C_t): Uses tanh activation\n",
        "            C_tilde = torch.tanh(torch.matmul(X, self.W_xc) +\n",
        "                                 torch.matmul(H, self.W_hc) + self.b_c)\n",
        "\n",
        "            # 5. Cell State Update (C_t) - The Core LSTM Operation\n",
        "            C = F * C + I * C_tilde\n",
        "\n",
        "            # 6. Hidden State Update (H_t)\n",
        "            H = O * torch.tanh(C)\n",
        "\n",
        "            outputs.append(H)\n",
        "\n",
        "        return outputs, (H, C)\n",
        "\n",
        "def load_text(path):\n",
        "    with open(path, \"r\", encoding='utf-8') as f:\n",
        "        return f.read()\n",
        "\n",
        "def build_vocab(text):\n",
        "    chars = sorted(set(text))\n",
        "    stoi = {ch: i for i, ch in enumerate(chars)}\n",
        "    itos = {i:ch for ch, i in stoi.items()}\n",
        "    return stoi, itos, len(chars)\n",
        "\n",
        "def one_hot(indices, vocab_size):\n",
        "    return torch.eye(vocab_size)[indices]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# comments\n",
        "def train(text, epochs=20, seq_len=40, hidden_size=128, lr=0.003):\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "    stoi, itos, vocab_size = build_vocab(text)\n",
        "    data = torch.tensor([stoi[c] for c in text], dtype=torch.long)\n",
        "\n",
        "    # ---- train / validation split ----\n",
        "    split = int(0.9 * len(data))\n",
        "    train_data = data[:split]\n",
        "    val_data = data[split:]\n",
        "\n",
        "    model = LSTMScratch(vocab_size, hidden_size).to(device)\n",
        "    output_layer = nn.Linear(hidden_size, vocab_size).to(device)\n",
        "\n",
        "    optimizer = torch.optim.Adam(\n",
        "        list(model.parameters()) + list(output_layer.parameters()), lr=lr\n",
        "    )\n",
        "\n",
        "    # üî• CRITICAL FIX\n",
        "    loss_fn = nn.CrossEntropyLoss(reduction=\"sum\")\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        total_loss = 0.0\n",
        "        total_tokens = 0\n",
        "        H_C = None\n",
        "\n",
        "        for step, i in enumerate(range(0, len(train_data) - seq_len - 1, seq_len)):\n",
        "            x_idx = train_data[i:i+seq_len]\n",
        "            y = train_data[i+1:i+seq_len+1]\n",
        "\n",
        "            x = one_hot(x_idx, vocab_size).to(device).unsqueeze(1)\n",
        "            y = y.to(device)\n",
        "\n",
        "            outputs, H_C = model(x, H_C)\n",
        "            H_C = (H_C[0].detach(), H_C[1].detach())\n",
        "\n",
        "            logits = torch.stack([output_layer(h) for h in outputs]).squeeze(1)\n",
        "            loss = loss_fn(logits, y)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            total_tokens += y.numel()\n",
        "\n",
        "            if step % 500 == 0:\n",
        "                print(\n",
        "                    f\"Epoch {epoch:02d} | Step {step:06d} | \"\n",
        "                    f\"Batch ppl: {math.exp(loss.item()/y.numel()):.2f}\",\n",
        "                    flush=True\n",
        "                )\n",
        "\n",
        "        train_ppl = math.exp(total_loss / total_tokens)\n",
        "\n",
        "        # ---- VALIDATION ----\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        val_tokens = 0\n",
        "        H_C = None\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for i in range(0, len(val_data) - seq_len - 1, seq_len):\n",
        "                x_idx = val_data[i:i+seq_len]\n",
        "                y = val_data[i+1:i+seq_len+1]\n",
        "\n",
        "                x = one_hot(x_idx, vocab_size).to(device).unsqueeze(1)\n",
        "                y = y.to(device)\n",
        "\n",
        "                outputs, H_C = model(x, H_C)\n",
        "                logits = torch.stack([output_layer(h) for h in outputs]).squeeze(1)\n",
        "\n",
        "                loss = loss_fn(logits, y)\n",
        "                val_loss += loss.item()\n",
        "                val_tokens += y.numel()\n",
        "\n",
        "        val_ppl = math.exp(val_loss / val_tokens)\n",
        "\n",
        "        print(f\"\\nEpoch {epoch:02d} DONE\")\n",
        "        print(f\"Train Perplexity: {train_ppl:.2f}\")\n",
        "        print(f\"Valid Perplexity: {val_ppl:.2f}\")\n",
        "\n",
        "        sample = generate_text(\n",
        "            model, output_layer, stoi, itos,\n",
        "            start_char=text[0],\n",
        "            device=device\n",
        "        )\n",
        "        print(\"\\n--- Sample ---\")\n",
        "        print(sample)\n",
        "        print(\"-\" * 60)"
      ],
      "metadata": {
        "id": "2_HTXjW6Oagp"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train(text, epochs=5, seq_len=40, hidden_size=128, lr=0.003)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dKxEnEUgN8fX",
        "outputId": "6c62e880-5fd9-4d54-efef-cedf197ebba9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 00 | Step 000000 | Batch ppl: 97.40\n",
            "Epoch 00 | Step 000500 | Batch ppl: 11.93\n",
            "Epoch 00 | Step 001000 | Batch ppl: 11.84\n",
            "Epoch 00 | Step 001500 | Batch ppl: 9.64\n",
            "Epoch 00 | Step 002000 | Batch ppl: 9.57\n",
            "Epoch 00 | Step 002500 | Batch ppl: 8.29\n",
            "Epoch 00 | Step 003000 | Batch ppl: 5.80\n",
            "Epoch 00 | Step 003500 | Batch ppl: 8.41\n",
            "Epoch 00 | Step 004000 | Batch ppl: 6.12\n",
            "Epoch 00 | Step 004500 | Batch ppl: 5.51\n",
            "Epoch 00 | Step 005000 | Batch ppl: 4.71\n",
            "Epoch 00 | Step 005500 | Batch ppl: 4.24\n",
            "Epoch 00 | Step 006000 | Batch ppl: 5.40\n",
            "Epoch 00 | Step 006500 | Batch ppl: 4.55\n",
            "Epoch 00 | Step 007000 | Batch ppl: 3.60\n",
            "Epoch 00 | Step 007500 | Batch ppl: 5.61\n",
            "Epoch 00 | Step 008000 | Batch ppl: 4.61\n",
            "Epoch 00 | Step 008500 | Batch ppl: 3.02\n",
            "Epoch 00 | Step 009000 | Batch ppl: 6.17\n",
            "Epoch 00 | Step 009500 | Batch ppl: 4.46\n",
            "Epoch 00 | Step 010000 | Batch ppl: 4.70\n",
            "Epoch 00 | Step 010500 | Batch ppl: 6.93\n",
            "Epoch 00 | Step 011000 | Batch ppl: 5.19\n",
            "Epoch 00 | Step 011500 | Batch ppl: 4.62\n",
            "Epoch 00 | Step 012000 | Batch ppl: 3.33\n",
            "Epoch 00 | Step 012500 | Batch ppl: 3.79\n",
            "Epoch 00 | Step 013000 | Batch ppl: 5.52\n",
            "Epoch 00 | Step 013500 | Batch ppl: 3.96\n",
            "Epoch 00 | Step 014000 | Batch ppl: 4.55\n",
            "Epoch 00 | Step 014500 | Batch ppl: 3.24\n",
            "Epoch 00 | Step 015000 | Batch ppl: 2.84\n",
            "Epoch 00 | Step 015500 | Batch ppl: 4.67\n",
            "Epoch 00 | Step 016000 | Batch ppl: 3.33\n",
            "Epoch 00 | Step 016500 | Batch ppl: 4.83\n",
            "\n",
            "Epoch 00 DONE\n",
            "Train Perplexity: 5.23\n",
            "Valid Perplexity: 5.63\n",
            "\n",
            "--- Sample ---\n",
            " doon Lizzy: if\n",
            "your concern with Dirne.\n",
            "Though Elizabeth world in demine not be of\n",
            "more daughter, you haven you tell particulations known to mind my raut of\n",
            "Londtas. But supphent a latt empar; and it will dost detinied him wish fram the young.\n",
            "\n",
            "[Illustration] le. Elizabeth will never really is it Ly\n",
            "------------------------------------------------------------\n",
            "Epoch 01 | Step 000000 | Batch ppl: 34.73\n",
            "Epoch 01 | Step 000500 | Batch ppl: 3.51\n",
            "Epoch 01 | Step 001000 | Batch ppl: 3.95\n",
            "Epoch 01 | Step 001500 | Batch ppl: 3.97\n",
            "Epoch 01 | Step 002000 | Batch ppl: 3.18\n",
            "Epoch 01 | Step 002500 | Batch ppl: 5.88\n",
            "Epoch 01 | Step 003000 | Batch ppl: 3.41\n",
            "Epoch 01 | Step 003500 | Batch ppl: 5.95\n",
            "Epoch 01 | Step 004000 | Batch ppl: 3.47\n",
            "Epoch 01 | Step 004500 | Batch ppl: 4.35\n",
            "Epoch 01 | Step 005000 | Batch ppl: 3.28\n",
            "Epoch 01 | Step 005500 | Batch ppl: 3.35\n",
            "Epoch 01 | Step 006000 | Batch ppl: 4.01\n",
            "Epoch 01 | Step 006500 | Batch ppl: 3.33\n",
            "Epoch 01 | Step 007000 | Batch ppl: 3.03\n",
            "Epoch 01 | Step 007500 | Batch ppl: 5.11\n",
            "Epoch 01 | Step 008000 | Batch ppl: 5.06\n",
            "Epoch 01 | Step 008500 | Batch ppl: 2.40\n",
            "Epoch 01 | Step 009000 | Batch ppl: 5.32\n",
            "Epoch 01 | Step 009500 | Batch ppl: 3.67\n",
            "Epoch 01 | Step 010000 | Batch ppl: 3.69\n",
            "Epoch 01 | Step 010500 | Batch ppl: 4.87\n",
            "Epoch 01 | Step 011000 | Batch ppl: 4.49\n",
            "Epoch 01 | Step 011500 | Batch ppl: 3.83\n",
            "Epoch 01 | Step 012000 | Batch ppl: 3.00\n",
            "Epoch 01 | Step 012500 | Batch ppl: 3.27\n",
            "Epoch 01 | Step 013000 | Batch ppl: 4.46\n",
            "Epoch 01 | Step 013500 | Batch ppl: 3.40\n",
            "Epoch 01 | Step 014000 | Batch ppl: 3.57\n",
            "Epoch 01 | Step 014500 | Batch ppl: 3.12\n",
            "Epoch 01 | Step 015000 | Batch ppl: 2.53\n",
            "Epoch 01 | Step 015500 | Batch ppl: 3.67\n",
            "Epoch 01 | Step 016000 | Batch ppl: 2.80\n",
            "Epoch 01 | Step 016500 | Batch ppl: 4.41\n",
            "\n",
            "Epoch 01 DONE\n",
            "Train Perplexity: 3.81\n",
            "Valid Perplexity: 5.27\n",
            "\n",
            "--- Sample ---\n",
            " doing uponous between spyeface that would away to obligubted respiviable. They are till _its_ so mare. Ares darcal would not be that Darcy!--mitawing even _me_; the either made no mote and laffer, that not this, to my coming joined\n",
            "nefuse my considerment. And allus my so. I am me secmiling for my ce\n",
            "------------------------------------------------------------\n",
            "Epoch 02 | Step 000000 | Batch ppl: 30.38\n",
            "Epoch 02 | Step 000500 | Batch ppl: 3.49\n",
            "Epoch 02 | Step 001000 | Batch ppl: 3.45\n",
            "Epoch 02 | Step 001500 | Batch ppl: 3.42\n",
            "Epoch 02 | Step 002000 | Batch ppl: 3.00\n",
            "Epoch 02 | Step 002500 | Batch ppl: 5.64\n",
            "Epoch 02 | Step 003000 | Batch ppl: 3.27\n",
            "Epoch 02 | Step 003500 | Batch ppl: 4.96\n",
            "Epoch 02 | Step 004000 | Batch ppl: 3.43\n",
            "Epoch 02 | Step 004500 | Batch ppl: 3.62\n",
            "Epoch 02 | Step 005000 | Batch ppl: 3.07\n",
            "Epoch 02 | Step 005500 | Batch ppl: 3.12\n",
            "Epoch 02 | Step 006000 | Batch ppl: 4.03\n",
            "Epoch 02 | Step 006500 | Batch ppl: 3.04\n",
            "Epoch 02 | Step 007000 | Batch ppl: 2.91\n",
            "Epoch 02 | Step 007500 | Batch ppl: 4.13\n",
            "Epoch 02 | Step 008000 | Batch ppl: 5.61\n",
            "Epoch 02 | Step 008500 | Batch ppl: 2.19\n",
            "Epoch 02 | Step 009000 | Batch ppl: 5.51\n",
            "Epoch 02 | Step 009500 | Batch ppl: 3.34\n",
            "Epoch 02 | Step 010000 | Batch ppl: 3.63\n",
            "Epoch 02 | Step 010500 | Batch ppl: 4.28\n",
            "Epoch 02 | Step 011000 | Batch ppl: 3.99\n",
            "Epoch 02 | Step 011500 | Batch ppl: 3.46\n",
            "Epoch 02 | Step 012000 | Batch ppl: 2.88\n",
            "Epoch 02 | Step 012500 | Batch ppl: 3.19\n",
            "Epoch 02 | Step 013000 | Batch ppl: 4.37\n",
            "Epoch 02 | Step 013500 | Batch ppl: 3.38\n",
            "Epoch 02 | Step 014000 | Batch ppl: 3.38\n",
            "Epoch 02 | Step 014500 | Batch ppl: 2.97\n",
            "Epoch 02 | Step 015000 | Batch ppl: 2.49\n",
            "Epoch 02 | Step 015500 | Batch ppl: 3.47\n",
            "Epoch 02 | Step 016000 | Batch ppl: 2.73\n",
            "Epoch 02 | Step 016500 | Batch ppl: 4.29\n",
            "\n",
            "Epoch 02 DONE\n",
            "Train Perplexity: 3.59\n",
            "Valid Perplexity: 5.20\n",
            "\n",
            "--- Sample ---\n",
            " will me forget it a shors?‚Äù\n",
            "\n",
            "‚ÄúThere more!‚Äù\n",
            "\n",
            "‚ÄúTo reply to him, and proceede to _cch_miness, if they reprosed to you that never\n",
            "to mave young your yoursele of their character. I am before your rese well, with part of such anschement, I walk you met entipency of Jane and disaffaire.\n",
            "\n",
            "A grameful and the\n",
            "------------------------------------------------------------\n",
            "Epoch 03 | Step 000000 | Batch ppl: 39.85\n",
            "Epoch 03 | Step 000500 | Batch ppl: 4.11\n",
            "Epoch 03 | Step 001000 | Batch ppl: 3.30\n",
            "Epoch 03 | Step 001500 | Batch ppl: 3.08\n",
            "Epoch 03 | Step 002000 | Batch ppl: 2.92\n",
            "Epoch 03 | Step 002500 | Batch ppl: 5.85\n",
            "Epoch 03 | Step 003000 | Batch ppl: 3.27\n",
            "Epoch 03 | Step 003500 | Batch ppl: 4.97\n",
            "Epoch 03 | Step 004000 | Batch ppl: 3.15\n",
            "Epoch 03 | Step 004500 | Batch ppl: 3.71\n",
            "Epoch 03 | Step 005000 | Batch ppl: 2.77\n",
            "Epoch 03 | Step 005500 | Batch ppl: 3.19\n",
            "Epoch 03 | Step 006000 | Batch ppl: 3.53\n",
            "Epoch 03 | Step 006500 | Batch ppl: 2.78\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_FrnXlxcH-ej"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}